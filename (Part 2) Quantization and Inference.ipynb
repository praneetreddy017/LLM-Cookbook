{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c093dcd4",
   "metadata": {},
   "source": [
    "### Quantization of a fine-tuned LLM and Inferencing using the quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd17a8",
   "metadata": {},
   "source": [
    "LLMs are computationally expensive to run. This is where quantization comes in. Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).\n",
    "\n",
    "For more information regarding quantization, please read this: https://huggingface.co/docs/optimum/concept_guides/quantization\n",
    "\n",
    "\n",
    "In this notebook, we perform Quantization using the package ctranslate2. ctranslate2 implements a custom runtime solution that allows for 8 bit quantization of an LLM to run on a CPU unlike the base hugging face model object (that requires the package bitsandbytes - not supported on Windows 10 and 11)\n",
    "\n",
    "Models supported by ctranslate2 are listed here: https://github.com/OpenNMT/CTranslate2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1a341",
   "metadata": {},
   "source": [
    "Packages used in this notebook are as follows:\n",
    "1. adapter-transformers\n",
    "2. ctranslate2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624a0f9",
   "metadata": {},
   "source": [
    "Quantization is done in the command prompt. To run command prompt commands, we prepend the cell with an \"!\" symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc33a6",
   "metadata": {},
   "source": [
    "The syntax for the command is as follows:<br>\n",
    "!ct2-transformers-converter --model \"model path\" --output_dir \"output_path\" --quantization int8<br>\n",
    "Supported Quantization options are listed here: https://opennmt.net/CTranslate2/quantization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dff8c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|#####     | 1/2 [01:42<01:42, 102.29s/it]\n",
      "Loading checkpoint shards: 100%|##########| 2/2 [01:48<00:00, 45.61s/it] \n",
      "Loading checkpoint shards: 100%|##########| 2/2 [01:48<00:00, 54.11s/it]\n"
     ]
    }
   ],
   "source": [
    "!ct2-transformers-converter --model \"C:\\Users\\JkReddy\\Desktop\\WCM - Volunteer Work\\Homeless\\tuned_model\\flan-t5-xl\" --output_dir \"C:\\Users\\JkReddy\\Desktop\\WCM - Volunteer Work\\Homeless\\tuned_model\\quantized_tuned_flan_t5_xl\" --quantization int8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17e551",
   "metadata": {},
   "source": [
    "Loading the model using ctranslate2 package\n",
    "The translator function can be used for our Q&A task (odd name but trust me, heard the dev saying on a reddit thread that we should use the Translator here for this model. If Translator does not work for your model, use generator). For Seq2Seq models we probably have to use the Translator class and its methods. Generator class probably works for Causal language models. Will update this notebook once I look into the documentation for ctranslate2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b79d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ctranslate2\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "translator = ctranslate2.Translator(r\"C:\\Users\\JkReddy\\Desktop\\WCM - Volunteer Work\\Homeless\\tuned_model\\quantized_tuned_flan_t5_xl\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db15c4",
   "metadata": {},
   "source": [
    "Executing a sample question to see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a3f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Read what the Clinician said in the Context below and answer the Question by choosing from the below provided Choices.\n",
      "Context: The Clinician said, \"He is sympathetic to the homeless\"\n",
      "Question: \"In the clinician's opinon, was the person themself homeless?\"\n",
      "Choices: Yes; No\n",
      "Answer:\n",
      "\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "input_text =  \"\"\"Instruction: Read what the Clinician said in the Context below and answer the Question by choosing from the below provided Choices.\n",
    "Context: The Clinician said, \"He is sympathetic to the homeless\"\n",
    "Question: \"In the clinician's opinon, was the person themself homeless?\"\n",
    "Choices: Yes; No\n",
    "Answer:\n",
    "\"\"\"\n",
    "print(input_text)\n",
    "\n",
    "\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "results = translator.translate_batch([input_tokens])\n",
    "output_tokens = results[0].hypotheses[0]\n",
    "output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6684a12",
   "metadata": {},
   "source": [
    "For more information about the ctranslate2 package and its functionalities, please have a look at this: https://opennmt.net/CTranslate2/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
